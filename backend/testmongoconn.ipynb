{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "Using cache found in /Users/junjie/.cache/torch/hub/snakers4_silero-vad_master\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VAD...\n",
      "Splitting by silence found 60 utterances\n",
      "Extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utterances: 100%|██████████| 60/60 [00:00<00:00, 102.37it/s]\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/sklearn/manifold/_spectral_embedding.py:310: UserWarning: Array is not symmetric, and will be converted to symmetric by average with its transpose.\n",
      "  adjacency = check_symmetric(adjacency)\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering to 2 speakers...\n",
      "Cleaning up output...\n",
      "Done!\n",
      "Inserted document with ID: 676a839fc7c0f5b9c5fd5fad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a83a5c7c0f5b9c5fd5fae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a83e6c7c0f5b9c5fd5faf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a83f1c7c0f5b9c5fd5fb0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8454c7c0f5b9c5fd5fb1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8466c7c0f5b9c5fd5fb2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8471c7c0f5b9c5fd5fb3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8477c7c0f5b9c5fd5fb4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a847dc7c0f5b9c5fd5fb5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8488c7c0f5b9c5fd5fb6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8493c7c0f5b9c5fd5fb7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a8499c7c0f5b9c5fd5fb8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1:\n",
      "Text:  So as mentioned, audio preprocessing, speech diarization, then as I mentioned audio preprocessing.\n",
      "Timestamp: 4.10s to 8.49s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  do it like a front end.  So, and you also provided me with a research paper to work on.  Um...  So for this...  We should have created like a React front then. So.  I have two portions, one for file upload.  Uh...  and one for all.  Shutting.  Yeah.  This is the end of the video. Thanks for watching.\n",
      "Timestamp: 8.49s to 39.29s\n",
      "\n",
      "Speaker 1:\n",
      "Text:  to ask questions.  So let's see.\n",
      "Timestamp: 39.59s to 42.08s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  If I  I blew it.  I'll foul now over here.  And I try. Hope it works. Yep.  Okay, so.   I forgot my IP.  It should reflect here accordingly. So this is just part of the pipeline.  Yeah.  So once I do this, it should propagate to...  Bye.  It should basically process the file.  And then.  propagate all the way to no sequel.  But for that I haven't linked to the...  vector db.  So that.\n",
      "Timestamp: 42.31s to 95.39s\n",
      "\n",
      "Speaker 1:\n",
      "Text:  So this frontend is the first thing I've done.  Then regarding transcription.  Regarding tr- okay actually\n",
      "Timestamp: 95.65s to 108.51s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  I don't think I have the result here but I did.  Changde.\n",
      "Timestamp: 108.80s to 112.51s\n",
      "\n",
      "Speaker 1:\n",
      "Text:  I did do some semantic chunking.\n",
      "Timestamp: 112.77s to 114.37s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  for the process.\n",
      "Timestamp: 114.69s to 115.81s\n",
      "\n",
      "Speaker 1:\n",
      "Text:   transcription.\n",
      "Timestamp: 115.81s to 117.53s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  and...  I use eyeballing glasses.\n",
      "Timestamp: 118.43s to 121.51s\n",
      "\n",
      "Speaker 1:\n",
      "Text:  basically the results that I saw.\n",
      "Timestamp: 121.51s to 123.45s\n",
      "\n",
      "Speaker 0:\n",
      "Text:  the chunks I saw.  Dewa!  Yeah, they were indeed like semantically chunked for the most part.  but  It's not entirely like...  totally accurate.  but it is pretty accurate in terms of like, let's say my Changwan, it'll go by like one topic.  then Changtung will be another topic of the convo.  it's able to register that pretty well.  Yeah.\n",
      "Timestamp: 123.78s to 145.95s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import whisper\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "from simple_diarizer.diarizer import Diarizer\n",
    "from config import MONGODB_URI, MONGODB_DATABASE_NAME, MONGODB_COLLECTION_NAME\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the Whisper model for transcription\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "\n",
    "def process_audio(file_path):\n",
    "    # Load and denoise the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.9, stationary=True)\n",
    "    \n",
    "    # Save the denoised audio to a temporary file\n",
    "    denoised_audio_file = '/tmp/denoised_audio.wav'\n",
    "    sf.write(denoised_audio_file, reduced_noise_audio, sr)\n",
    "\n",
    "    # Perform speaker diarization using simple_diarizer\n",
    "    diarization = Diarizer(embed_model='xvec', cluster_method='sc')\n",
    "    segments = diarization.diarize(denoised_audio_file, num_speakers=2)\n",
    "\n",
    "    # Process each speaker segment and transcribe\n",
    "    speaker_transcriptions = []\n",
    "    current_speaker = None\n",
    "\n",
    "    # Reload the denoised audio data for segmentation\n",
    "    audio, sr = librosa.load(denoised_audio_file, sr=None)\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(MONGODB_URI)\n",
    "    db = client[MONGODB_DATABASE_NAME]\n",
    "    collection = db[MONGODB_COLLECTION_NAME]\n",
    "\n",
    "    for segment in segments:\n",
    "        start_time = segment['start']\n",
    "        end_time = segment['end']\n",
    "        speaker_label = segment['label']  # The speaker label (0, 1, etc.)\n",
    "\n",
    "        # Convert the start and end times from seconds to sample indices\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        # Extract the audio segment corresponding to the current speaker's time frame\n",
    "        segment_audio = audio[start_sample:end_sample]\n",
    "\n",
    "        # Save the extracted audio segment to a temporary file for Whisper to transcribe\n",
    "        temp_segment_path = f\"/tmp/temp_speaker_{speaker_label}_{int(start_time)}.wav\"\n",
    "        sf.write(temp_segment_path, segment_audio, sr)\n",
    "\n",
    "        # Transcribe the audio segment using Whisper, forcing it to use English language\n",
    "        transcription_result = whisper_model.transcribe(temp_segment_path, language=\"en\")\n",
    "        transcription_text = transcription_result[\"text\"]\n",
    "\n",
    "        # Handle the first transcription block\n",
    "        if current_speaker is None or current_speaker != speaker_label:\n",
    "            # Add the new transcription block for the first speaker\n",
    "            transcription_document = {\n",
    "                \"speaker\": int(speaker_label),  # Convert numpy.int32 to native int\n",
    "                \"text\": transcription_text,\n",
    "                \"start_time\": float(start_time),  # Convert to float for MongoDB\n",
    "                \"end_time\": float(end_time)       # Convert to float for MongoDB\n",
    "            }\n",
    "            # Insert the transcription document into MongoDB\n",
    "            result = collection.insert_one(transcription_document)\n",
    "            print(f\"Inserted document with ID: {result.inserted_id}\")\n",
    "\n",
    "            # Add the transcription block to the list\n",
    "            speaker_transcriptions.append({\n",
    "                \"speaker\": speaker_label,\n",
    "                \"text\": transcription_text,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time\n",
    "            })\n",
    "        else:\n",
    "            # If the same speaker is continuing, append the transcription and extend the end time\n",
    "            speaker_transcriptions[-1][\"text\"] += \" \" + transcription_text\n",
    "            speaker_transcriptions[-1][\"end_time\"] = float(end_time)\n",
    "\n",
    "        # Delete the temporary file after processing\n",
    "        os.remove(temp_segment_path)\n",
    "\n",
    "        # Update the current speaker for the next iteration\n",
    "        current_speaker = speaker_label\n",
    "\n",
    "    # Delete the denoised audio file after processing\n",
    "    os.remove(denoised_audio_file)\n",
    "\n",
    "    # Output the speaker transcriptions with timestamps\n",
    "    for block in speaker_transcriptions:\n",
    "        print(f\"Speaker {block['speaker']}:\")\n",
    "        print(f\"Text: {block['text']}\")\n",
    "        print(f\"Timestamp: {block['start_time']:.2f}s to {block['end_time']:.2f}s\\n\")\n",
    "\n",
    "    return speaker_transcriptions\n",
    "\n",
    "# Add the if __name__ == \"__main__\" block here:\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the file path from the command-line arguments\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python script.py <file_path>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    file_path = '/Users/junjie/mmRag/backend/SIT_NVIDIA_MEETING_PART1.wav' # Get the file path from the first argument\n",
    "    process_audio(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This portion i want to use openai gpt 4o mini to output me the text in the following format which i will then use to store in mongodb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "Using cache found in /Users/junjie/.cache/torch/hub/snakers4_silero-vad_master\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VAD...\n",
      "Splitting by silence found 8 utterances\n",
      "Extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utterances: 100%|██████████| 8/8 [00:00<00:00, 96.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering to 2 speakers...\n",
      "Cleaning up output...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/sklearn/manifold/_spectral_embedding.py:310: UserWarning: Array is not symmetric, and will be converted to symmetric by average with its transpose.\n",
      "  adjacency = check_symmetric(adjacency)\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 676a900fc7c0f5b9c5fd5fbc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import whisper\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "from simple_diarizer.diarizer import Diarizer\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "from config import MONGODB_URI, MONGODB_DATABASE_NAME, MONGODB_COLLECTION_NAME, OPENAI_API_KEY\n",
    "from openai import OpenAI  # You need to install openai library for GPT-3 or GPT-4 integration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "# OPENAI_API_KEY_4O = os.getenv(OPENAI_API_KEY)\n",
    "# Load the Whisper model for transcription\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "\n",
    "\n",
    "# OpenAI API Key Configuration (Replace with your actual key)\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def generate_summary_and_action_items(transcription_text):\n",
    "    # Define the prompt to extract summary, action items, and decisions\n",
    "    prompt = f\"\"\"\n",
    "    You are given a transcript of a meeting, with dialogue between different speakers. Your task is to summarize the content covered in the meeting and extract the action items in the format below. \n",
    "    Please focus on the key topics discussed, decisions made, and action items that need to be followed up on. The format should include:\n",
    "    \n",
    "    Meeting Summary: A high-level summary of the key topics discussed in the meeting.\n",
    "    Action Items: A list of actionable tasks, including who is responsible for each task and when it needs to be completed.\n",
    "    Decisions Made: A list of any decisions that were made during the meeting.\n",
    "    \n",
    "    The input for this prompt will be in the format here:\n",
    "\n",
    "    {transcription_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call OpenAI GPT model to generate structured content (summary, action items, decisions)\n",
    "    response = client.chat.completions.create(\n",
    "        \n",
    "        model=\"gpt-4o-mini\",  # Use \"gpt-3.5-turbo\" or \"gpt-4\" if available\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=16384,  # You can adjust the token limit based on your needs\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def store_meeting_data(file_id, file_name, file_content):\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(MONGODB_URI)\n",
    "    db = client[MONGODB_DATABASE_NAME]\n",
    "    collection = db[MONGODB_COLLECTION_NAME]\n",
    "    \n",
    "    # Create the document to store in MongoDB\n",
    "    meeting_data = {\n",
    "        \"file_id\": file_id,\n",
    "        \"file_name\": file_name,\n",
    "        \"file_content\": file_content,\n",
    "        \"timestamp\": os.path.getmtime(file_name)  # You can use the file modification time as a timestamp\n",
    "    }\n",
    "    \n",
    "    # Insert the meeting data into MongoDB\n",
    "    result = collection.insert_one(meeting_data)\n",
    "    print(f\"Inserted document with ID: {result.inserted_id}\")\n",
    "\n",
    "def process_audio(file_path):\n",
    "    # Load and denoise the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    reduced_noise_audio = nr.reduce_noise(y=audio, sr=sr, prop_decrease=0.9, stationary=True)\n",
    "    \n",
    "    # Save the denoised audio to a temporary file\n",
    "    denoised_audio_file = '/tmp/denoised_audio.wav'\n",
    "    sf.write(denoised_audio_file, reduced_noise_audio, sr)\n",
    "\n",
    "    # Perform speaker diarization using simple_diarizer\n",
    "    diarization = Diarizer(embed_model='xvec', cluster_method='sc')\n",
    "    segments = diarization.diarize(denoised_audio_file, num_speakers=2)\n",
    "\n",
    "    # Process each speaker segment and transcribe\n",
    "    speaker_transcriptions = []\n",
    "    current_speaker = None\n",
    "\n",
    "    # Reload the denoised audio data for segmentation\n",
    "    audio, sr = librosa.load(denoised_audio_file, sr=None)\n",
    "\n",
    "    # Generate the transcription text\n",
    "    transcription_text = \"\"\n",
    "    for segment in segments:\n",
    "        start_time = segment['start']\n",
    "        end_time = segment['end']\n",
    "        speaker_label = segment['label']\n",
    "\n",
    "        # Convert the start and end times from seconds to sample indices\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        # Extract the audio segment corresponding to the current speaker's time frame\n",
    "        segment_audio = audio[start_sample:end_sample]\n",
    "\n",
    "        # Save the extracted audio segment to a temporary file for Whisper to transcribe\n",
    "        temp_segment_path = f\"/tmp/temp_speaker_{speaker_label}_{int(start_time)}.wav\"\n",
    "        sf.write(temp_segment_path, segment_audio, sr)\n",
    "\n",
    "        # Transcribe the audio segment using Whisper, forcing it to use English language\n",
    "        transcription_result = whisper_model.transcribe(temp_segment_path, language=\"en\")\n",
    "        transcription_text += transcription_result[\"text\"] + \" \"\n",
    "\n",
    "        # Delete the temporary file after processing\n",
    "        os.remove(temp_segment_path)\n",
    "\n",
    "        # Update the current speaker for the next iteration\n",
    "        current_speaker = speaker_label\n",
    "\n",
    "    # Delete the denoised audio file after processing\n",
    "    os.remove(denoised_audio_file)\n",
    "\n",
    "    # Generate the structured summary and action items using the transcription text\n",
    "    structured_content = generate_summary_and_action_items(transcription_text)\n",
    "\n",
    "    # Prepare to store the meeting data in MongoDB\n",
    "    file_id = os.path.basename(file_path)  # You can use file name or generate a unique ID\n",
    "    file_name = file_path\n",
    "    store_meeting_data(file_id, file_name, structured_content)\n",
    "\n",
    "# Add the if __name__ == \"__main__\" block here:\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the file path from the command-line arguments\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python script.py <file_path>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    file_path = '/Users/junjie/mmRag/backend/PART2.wav'  # Get the file path from the first argument\n",
    "    process_audio(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjie/mmRag/myVenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"mm_collection\",\n",
    "    vectors_config=VectorParams(size=4, distance=Distance.DOT),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/mm_collection \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"mm_collection\",\n",
    "    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# construct vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name='mm_collection',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "import uuid\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Example documents to embed\n",
    "documents = [\n",
    "    \"Meeting 1 summary: We discussed project deadlines and assigned tasks.\",\n",
    "    \"Meeting 2 summary: The budget was approved, and the project manager was assigned.\",\n",
    "    # Add your other meeting documents here\n",
    "]\n",
    "\n",
    "# Generate embeddings (vectors) for each document\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "# Insert vectors into Qdrant\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),  # Unique ID for each document\n",
    "        vector=embedding.tolist(),  # Convert embedding to list format\n",
    "        payload={\"text\": documents[i]},  # Store the original document as payload\n",
    "    )\n",
    "    client.upsert(\n",
    "        collection_name=\"mm_collection\",\n",
    "        points=[point]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"text\": \"The meeting focused on the quality of audio files and their impact on processing accuracy.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Meeting Summary\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Investigate methods to improve audio file quality and reduce noise.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Action Items\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Review the current denoising techniques being used and assess their effectiveness.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Action Items\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"It was agreed that audio file quality is critical for achieving accurate results.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Decisions Made\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from config import OPENAI_API_KEY, WHISPER_MODEL, DIARIZER\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def semantic_chunker(transcription_text):\n",
    "    \"\"\"\n",
    "    This function sends the transcription text to an LLM, asking it to break the text into \n",
    "    semantically meaningful chunks, and returns the output in a JSON format.\n",
    "    \n",
    "    Parameters:\n",
    "        transcription_text (str): The text of the meeting transcription.\n",
    "    \n",
    "    Returns:\n",
    "        str: A JSON string containing the semantically chunked text with metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are given a transcript of a meeting with dialogue between different speakers. \n",
    "    Your task is to break the conversation into meaningful chunks and provide the output in JSON format.\n",
    "    Each chunk should have a \"text\" field containing the chunk content and a \"metadata\" field with relevant details like category (e.g., Meeting Summary, Action Items, etc.).\n",
    "\n",
    "    The format should be:\n",
    "    [\n",
    "        {{\n",
    "            \"text\": \"chunk text here\",\n",
    "            \"metadata\": {{\n",
    "                \"category\": \"category name here\"\n",
    "            }}\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Here's the meeting transcription text:\n",
    "\n",
    "    {transcription_text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Send the prompt to the LLM model (assuming it's set up for edge functions)\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=16384,  # Adjust the token limit based on your needs\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        refined_text = response.choices[0].message.content\n",
    "        return refined_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return json.dumps({\"error\": \"An error occurred during semantic chunking.\"})\n",
    "\n",
    "# Example usage\n",
    "transcription_text = \"\"\"\n",
    "Meeting Summary: The meeting focused on the quality of audio files and their impact on processing accuracy.\n",
    "\n",
    "Action Items:\n",
    "1. Investigate methods to improve audio file quality and reduce noise.\n",
    "2. Review the current denoising techniques being used and assess their effectiveness.\n",
    "\n",
    "Decisions Made: It was agreed that audio file quality is critical for achieving accurate results.\n",
    "\"\"\"\n",
    "\n",
    "print(semantic_chunker(transcription_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"text\": \"Okay, basically pre-processing. I looked at denoising which you mentioned. I also looked at... I learned some new terms. Something about amplitude. Like you want to scale it or something. Yeah, and another thing called normalization. So these are the three things I tried to do.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Discussion\",\n",
      "            \"speaker\": \"Speaker 0\",\n",
      "            \"filename\": \"/tmp/PART5.wav\",\n",
      "            \"timestamp\": \"2025-01-03\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"You mentioned denoiser library I tried that but I don't know whether it's because I did it wrongly. So the results...\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Discussion\",\n",
      "            \"speaker\": \"Speaker 0\",\n",
      "            \"filename\": \"/tmp/PART5.wav\",\n",
      "            \"timestamp\": \"2025-01-03\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"The audio file I eventually got, which I'm not gonna play here because it destroys my ears. Basically it's not good. I tried with two other libraries, notably is noise reduced.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Discussion\",\n",
      "            \"speaker\": \"Speaker 0\",\n",
      "            \"filename\": \"/tmp/PART5.wav\",\n",
      "            \"timestamp\": \"2025-01-03\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Where's my foul? I... I don't know it's my fault.\",\n",
      "        \"metadata\": {\n",
      "            \"category\": \"Clarification\",\n",
      "            \"speaker\": \"Speaker 1\",\n",
      "            \"filename\": \"/tmp/PART5.wav\",\n",
      "            \"timestamp\": \"2025-01-03\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def clean_file_content(file_content):\n",
    "    \"\"\"\n",
    "    Cleans the given file_content by removing unnecessary characters \n",
    "    and extracting the JSON data.\n",
    "\n",
    "    Parameters:\n",
    "        file_content (str): The raw file_content with extra characters.\n",
    "\n",
    "    Returns:\n",
    "        dict: A cleaned dictionary containing the parsed JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove the enclosing ```json and backticks\n",
    "        cleaned_content = file_content.strip('```json').strip('```')\n",
    "        \n",
    "        # Remove newlines and excess whitespace\n",
    "        cleaned_content = cleaned_content.replace(\"\\n\", \"\").replace(\"\\\\n\", \"\").strip()\n",
    "        \n",
    "        # Parse the cleaned JSON string into a Python dictionary\n",
    "        parsed_content = json.loads(cleaned_content)\n",
    "        \n",
    "        return parsed_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning file_content: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_content = \"\"\"```json\\n[\\n    {\\n        \\\"text\\\": \\\"Okay, basically pre-processing. I looked at denoising which you mentioned. I also looked at... I learned some new terms. Something about amplitude. Like you want to scale it or something. Yeah, and another thing called normalization. So these are the three things I tried to do.\\\",\\n        \\\"metadata\\\": {\\n            \\\"category\\\": \\\"Discussion\\\",\\n            \\\"speaker\\\": \\\"Speaker 0\\\",\\n            \\\"filename\\\": \\\"/tmp/PART5.wav\\\",\\n            \\\"timestamp\\\": \\\"2025-01-03\\\"\\n        }\\n    },\\n    {\\n        \\\"text\\\": \\\"You mentioned denoiser library I tried that but I don't know whether it's because I did it wrongly. So the results...\\\",\\n        \\\"metadata\\\": {\\n            \\\"category\\\": \\\"Discussion\\\",\\n            \\\"speaker\\\": \\\"Speaker 0\\\",\\n            \\\"filename\\\": \\\"/tmp/PART5.wav\\\",\\n            \\\"timestamp\\\": \\\"2025-01-03\\\"\\n        }\\n    },\\n    {\\n        \\\"text\\\": \\\"The audio file I eventually got, which I'm not gonna play here because it destroys my ears. Basically it's not good. I tried with two other libraries, notably is noise reduced.\\\",\\n        \\\"metadata\\\": {\\n            \\\"category\\\": \\\"Discussion\\\",\\n            \\\"speaker\\\": \\\"Speaker 0\\\",\\n            \\\"filename\\\": \\\"/tmp/PART5.wav\\\",\\n            \\\"timestamp\\\": \\\"2025-01-03\\\"\\n        }\\n    },\\n    {\\n        \\\"text\\\": \\\"Where's my foul? I... I don't know it's my fault.\\\",\\n        \\\"metadata\\\": {\\n            \\\"category\\\": \\\"Clarification\\\",\\n            \\\"speaker\\\": \\\"Speaker 1\\\",\\n            \\\"filename\\\": \\\"/tmp/PART5.wav\\\",\\n            \\\"timestamp\\\": \\\"2025-01-03\\\"\\n        }\\n    }\\n]\\n```\"\"\"\n",
    "\n",
    "cleaned_data = clean_file_content(file_content)\n",
    "\n",
    "# Print cleaned JSON data\n",
    "print(json.dumps(cleaned_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
